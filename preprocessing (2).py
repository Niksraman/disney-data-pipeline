# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wr8JBQqE3nJ_1a2PhfYlvjB4HA7QGXgn
"""

import pandas as pd
import re
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# 1. Load the Dataset
def load_dataset(file_path: str) -> pd.DataFrame:
    """
    Load data from a JSON file into a Pandas DataFrame.
    """
    logging.info(f"Loading dataset from {file_path}...")
    try:
        df = pd.read_json(file_path)
        logging.info(f"Dataset loaded successfully. Rows: {len(df)} Columns: {len(df.columns)}")
        return df
    except Exception as e:
        logging.error(f"Failed to load dataset: {e}")
        raise

# 2. Preprocessing Steps (as described before)
def handle_missing_values(df: pd.DataFrame, fill_values: dict) -> pd.DataFrame:
    logging.info("Handling missing values...")
    for column, value in fill_values.items():
        if column in df.columns:
            df[column] = df[column].fillna(value)
            logging.info(f"Filled missing values in '{column}' with '{value}'.")
    return df

def clean_text_column(df: pd.DataFrame, column: str) -> pd.DataFrame:
    logging.info(f"Cleaning text in column: {column}")
    if column in df.columns:
        df[column] = df[column].apply(lambda x: clean_text(x) if isinstance(x, str) else x)
    return df

def clean_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"#\w+", "", text)  # Remove hashtags
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    return text.strip()

def normalize_dates(df: pd.DataFrame, column: str) -> pd.DataFrame:
    logging.info(f"Normalizing dates in column: {column}")
    if column in df.columns:
        df[column] = pd.to_datetime(df[column], errors="coerce")
    return df

def validate_email(df: pd.DataFrame, column: str) -> pd.DataFrame:
    logging.info(f"Validating email addresses in column: {column}")
    if column in df.columns:
        df["email_valid"] = df[column].str.contains(r"^[\w\.-]+@[\w\.-]+\.\w+$", regex=True, na=False)
    return df

# 3. Save Preprocessed Data
def save_dataset(df: pd.DataFrame, output_path: str) -> None:
    """
    Save the preprocessed DataFrame to a JSON file.
    """
    logging.info(f"Saving preprocessed dataset to {output_path}...")
    df.to_json(output_path, orient="records", indent=4)
    logging.info("Preprocessed dataset saved successfully.")

# 4. Main Pipeline
def preprocess_data(file_path: str, output_path: str) -> None:
    """
    Complete pipeline to preprocess a JSON dataset.
    """
    # Load the dataset
    df = load_dataset(file_path)

    # Define preprocessing parameters
    fill_values = {"name": "Unknown", "description": "No description available."}
    text_columns = ["description"]
    date_columns = ["created_at"]
    email_column = "email"

    # Apply preprocessing
    df = handle_missing_values(df, fill_values)
    for column in text_columns:
        df = clean_text_column(df, column)
    for column in date_columns:
        df = normalize_dates(df, column)
    df = validate_email(df, email_column)

    # Save the preprocessed dataset
    save_dataset(df, output_path)

# Run the pipeline
if __name__ == "__main__":
    input_path = "data.json"  # Replace with your actual input file path
    output_path = "preprocessed_data.json"  # Replace with desired output file path
    preprocess_data(input_path, output_path)

"""1. Data Cleaning
Objective: Remove noise, inconsistencies, and invalid records.
Actions:
Remove unnecessary characters from text fields (e.g., hashtags, URLs, or special characters).
Standardize text formats (lowercase, consistent spacing).
Validate and filter invalid records (e.g., invalid email addresses).
2. Handle Missing Values
Objective: Avoid NULL values that can complicate querying.
Actions:
For critical fields (e.g., id, email): Drop rows if missing.
For non-critical fields (e.g., description, age): Impute defaults (e.g., "N/A" for text, mean/median for numerical values).
3. Data Normalization
Objective: Ensure consistency in the representation of data.
Actions:
Convert date fields to a consistent format (e.g., ISO 8601: YYYY-MM-DD).
Split complex fields (e.g., nested JSON) into separate columns.
"""