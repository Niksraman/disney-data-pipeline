# -*- coding: utf-8 -*-
"""Logging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GhrMZoEFJ5GBz3EWmEqMgGJjc3f3rmZ
"""

import pandas as pd
import re
import logging
import time
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from sqlalchemy import create_engine

# Configure Logging
def configure_logging(log_file="pipeline.log"):
    """
    Set up logging configuration.
    """
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        filename=log_file,
        filemode="w"  # Overwrite logs for each run
    )
    logging.info("Logging is configured.")

# Decorator for Execution Time Logging
def log_execution_time(func):
    """
    A decorator to log the execution time of functions.
    """
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        logging.info(f"{func.__name__} executed in {end_time - start_time:.2f} seconds.")
        return result
    return wrapper

# Database Configuration
DB_URL = "sqlite:///data_storage.db"
TABLE_NAME = "preprocessed_table"

# Load Dataset
@log_execution_time
def load_data(file_path):
    """
    Load a JSON dataset into a Pandas DataFrame.
    """
    logging.info(f"Loading dataset from: {file_path}")
    try:
        df = pd.read_json(file_path)
        logging.info(f"Dataset loaded successfully with {len(df)} rows and {len(df.columns)} columns.")
        return df
    except Exception as e:
        logging.error(f"Failed to load dataset: {e}")
        raise

# Text Preprocessing
@log_execution_time
def preprocess_text_column(df, column):
    """
    Clean and normalize text data in a specific column.
    """
    logging.info(f"Preprocessing text column: {column}")
    if column in df.columns:
        df[column] = df[column].apply(lambda x: clean_text(x) if isinstance(x, str) else x)
    return df

def clean_text(text):
    """
    Remove special characters, URLs, and punctuation from text.
    """
    text = text.lower()
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"#\w+", "", text)  # Remove hashtags
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    return text.strip()

# Handle Missing Values
@log_execution_time
def handle_missing_values(df, fill_values):
    """
    Fill missing values in the dataset.
    """
    logging.info("Handling missing values...")
    for column, value in fill_values.items():
        if column in df.columns:
            df[column] = df[column].fillna(value)
            logging.info(f"Filled missing values in '{column}' with '{value}'.")
    return df

# Normalize Dates
@log_execution_time
def normalize_dates(df, column):
    """
    Normalize date fields to a consistent format.
    """
    logging.info(f"Normalizing dates in column: {column}")
    if column in df.columns:
        df[column] = pd.to_datetime(df[column], errors="coerce")
    return df

# Generate Embeddings
@log_execution_time
def generate_embeddings(texts, model_name="sentence-transformers/all-MiniLM-L6-v2", batch_size=32):
    """
    Generate embeddings for a list of texts using a pre-trained model.
    """
    logging.info(f"Generating embeddings with model: {model_name}")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)
    logging.info(f"Generated embeddings for {len(texts)} texts.")
    return embeddings

# Store Data in Database
@log_execution_time
def store_data(df, table_name):
    """
    Store preprocessed data in a relational database.
    """
    logging.info(f"Storing data in database table: {table_name}")
    engine = create_engine(DB_URL)
    with engine.connect() as conn:
        df.to_sql(table_name, conn, if_exists="replace", index=False)
    logging.info(f"Data stored successfully in table: {table_name}")

# Store Embeddings in FAISS
@log_execution_time
def store_in_faiss(embeddings, ids, index_path="vector_index.index"):
    """
    Store embeddings in a FAISS index.
    """
    logging.info("Storing embeddings in FAISS index.")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)

    # Add embeddings with IDs
    index = faiss.IndexIDMap(index)
    index.add_with_ids(embeddings, np.array(ids).astype("int64"))

    # Save the index to disk
    faiss.write_index(index, index_path)
    logging.info(f"FAISS index saved to: {index_path}")

# Complete Preprocessing Pipeline
@log_execution_time
def preprocessing_pipeline(file_path, table_name, index_path):
    """
    Full preprocessing pipeline with logging.
    """
    logging.info("Starting preprocessing pipeline...")

    # Load data
    df = load_data(file_path)

    # Define preprocessing parameters
    fill_values = {"name": "Unknown", "description": "No description available."}

    # Preprocessing steps
    df = preprocess_text_column(df, "description")
    df = handle_missing_values(df, fill_values)
    df = normalize_dates(df, "created_at")

    # Store processed data in the database
    store_data(df, table_name)

    # Generate embeddings and store in FAISS
    embeddings = generate_embeddings(df["description"].tolist())
    store_in_faiss(embeddings, df["id"].tolist(), index_path)

    logging.info("Preprocessing pipeline completed successfully.")

# Main Function
if __name__ == "__main__":
    configure_logging()
    input_path = "data.json"
    output_index_path = "vector_index.index"
    preprocessing_pipeline(input_path, TABLE_NAME, output_index_path)