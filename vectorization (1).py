# -*- coding: utf-8 -*-
"""Vectorization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w2-UzGVbSnuIpQ1b8Urd2fJ310RIRma1
"""

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")

# 1. Load Data
def load_data(file_path):
    """
    Load JSON data into a Pandas DataFrame.
    """
    logging.info(f"Loading data from {file_path}...")
    df = pd.read_json(file_path)
    return df

# 2. Generate Embeddings
def generate_embeddings(texts, model_name="sentence-transformers/all-MiniLM-L6-v2", batch_size=32):
    """
    Generate embeddings for a list of texts using a pre-trained model.
    """
    logging.info(f"Loading model: {model_name}")
    model = SentenceTransformer(model_name)

    logging.info("Generating embeddings...")
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)
    logging.info("Embeddings generated successfully.")
    return embeddings

# 3. Store Embeddings in FAISS
def store_in_faiss(embeddings, ids, index_path="vector_index.index"):
    """
    Store embeddings in a FAISS index with corresponding IDs.
    """
    logging.info("Storing embeddings in FAISS...")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)

    # Add embeddings to FAISS index
    if ids is not None:
        index = faiss.IndexIDMap(index)
        index.add_with_ids(embeddings, np.array(ids).astype('int64'))
    else:
        index.add(embeddings)

    # Save the index
    faiss.write_index(index, index_path)
    logging.info(f"FAISS index saved to {index_path}")

# 4. Batch Processing
def process_in_batches(df, text_column, batch_size, model_name, index_path):
    """
    Process text data in batches to generate embeddings and store them.
    """
    logging.info("Starting batch processing...")
    all_embeddings = []
    all_ids = []

    # Process in batches
    for start in range(0, len(df), batch_size):
        end = start + batch_size
        batch = df.iloc[start:end]
        texts = batch[text_column].tolist()
        ids = batch["id"].tolist()

        embeddings = generate_embeddings(texts, model_name=model_name, batch_size=batch_size)
        all_embeddings.append(embeddings)
        all_ids.extend(ids)

    # Combine all embeddings and store in FAISS
    all_embeddings = np.vstack(all_embeddings)
    store_in_faiss(all_embeddings, all_ids, index_path=index_path)

# Main Vectorization Pipeline
def vectorization_pipeline(file_path, text_column, index_path, batch_size=32, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    """
    Complete vectorization pipeline: Load data, generate embeddings, and store them.
    """
    # Load data
    df = load_data(file_path)

    # Filter rows with missing text
    df = df.dropna(subset=[text_column])

    # Process data in batches
    process_in_batches(df, text_column, batch_size, model_name, index_path)
    logging.info("Vectorization pipeline completed successfully.")

# Run the pipeline
if __name__ == "__main__":
    input_path = "data.json"  # Input data file
    text_column = "description"  # Column to vectorize
    index_path = "vector_index.index"  # Output FAISS index file

    vectorization_pipeline(
        file_path=input_path,
        text_column=text_column,
        index_path=index_path,
        batch_size=32,
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )